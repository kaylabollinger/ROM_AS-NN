{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Order Modeling using Shallow ReLU Networks with Grassmann Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we provide sample code to reproduce results described in \n",
    "<a name=\"ref-1\"/>[(Bollinger and Schaeffer, 2020)](#cite-bollinger2020reduced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pymanopt.manifolds import Grassmann\n",
    "from pymanopt import Problem\n",
    "from pymanopt.solvers import SteepestDescent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "The code provided in this notebook assumes data is stored in the following way:\n",
    "- Data is stored as .npy files, with each data points stored in rows.\n",
    "- There are 2 or 3 data files, with directories provided by user: \n",
    "    - input data, labeled throughout code as $y$\n",
    "    - output data, labeled throughout code as $v$\n",
    "    - (optional) derivative data, labeled throughout code as $dv$. (If no derivative information is available, just use None for the $dv$ directory and use either identity or random initialization for $U$.)\n",
    "    \n",
    "We have provided data files of this form in the \"raw_data\" folder. This provided data corresponds to the drag coefficient associated with the NACA0012 airfoil with respect to 18-shape parameters. If you would like to apply this model to a different data set, just make sure your data files conform to the structure described above and provide the appropriate data directories in lines 4-6 of the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define input/output/derivative data directories below.\n",
    "'''\n",
    "y_dir = 'raw_data/y.npy'\n",
    "v_dir = 'raw_data/v.npy'\n",
    "dv_dir = 'raw_data/dv.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and validation set size using the \"train_num\" and \"val_num\" variables defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 50\n",
    "val_num = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided in this notebook will choose \"train_num\" and \"val_num\" randomly chosen data points (without replacement, to ensure the training set and validation set do not share data points) from the data files referenced above. If you wish to fix the randomness in choosing data points (e.g. if you would like to test different hyperparameters while keeping your data sets fixed) uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f944222810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0) # toggle comment to fix the random choice of train and val data\n",
    "torch.random.manual_seed(0) # toggle comment to fix random initializion of parameters within the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyperparameters\n",
    "\n",
    "Below is the collection of hyperparameters used for this model:\n",
    "- init_type: how to initialize the active subspace $U$\n",
    "    - 'acs' = use derivative information to find best approximation of $U$\n",
    "    - 'id' = initialize with identity matrix\n",
    "    - 'rn' = initialize with random matrix\n",
    "- max_outIter: the number of outer iterations the model will perform\n",
    "- acs_dim: active subspace dimension ($k$)\n",
    "- hidden_dim: hidden layer dimension of the network\n",
    "- lr: learning rate for ADAM optimizer in Neural Network Subproblem\n",
    "- lr_decay: learning rate decay for ADAM optimizer in Neural Network Subproblem\n",
    "- l2_reg_scale: $\\ell_2$ regularization parameter ($\\lambda$)\n",
    "- batch_size: batch size used in Neural Network Subproblem\n",
    "- num_epoch: number of inner iterations (epochs) used in the Neural Network Subproblem ($N_\\theta$)\n",
    "- numPrint_epoch: number of inner iterations (epochs) at which to save current loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_type = 'acs' # ['acs','id','rn']\n",
    "max_outIter = 5\n",
    "\n",
    "acs_dim = 2\n",
    "hidden_dim = 10\n",
    "\n",
    "lr = 0.001\n",
    "lr_decay = 0.9\n",
    "l2_reg_scale = 1e-7\n",
    "\n",
    "batch_size = 16\n",
    "num_epoch = 5000\n",
    "numPrint_epoch = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating/Defining Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, and define training/validation data subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load raw_data. '''\n",
    "y = np.load(y_dir)\n",
    "v = np.load(v_dir)\n",
    "dv = np.load(dv_dir)\n",
    "\n",
    "''' Choose data points from raw data. '''\n",
    "tot_num = y.shape[0] # total number of data points to choose from\n",
    "ind = list(range(tot_num)) # define indices to choose data points\n",
    "np.random.shuffle(ind) # shuffle indices to choose data points at random\n",
    "\n",
    "# choose training and validation indices from shuffled list ind\n",
    "train_ind = ind[:train_num]; del ind[:train_num] # delete used indices\n",
    "val_ind = ind[:val_num]; del ind[:val_num] # delete used indices\n",
    "    \n",
    "# choose training and validation data points defined by their respective indices\n",
    "train_y = y[train_ind,:]; train_v = v[train_ind,:]; train_dv = dv[train_ind,:]\n",
    "val_y = y[val_ind,:] ; val_v = v[val_ind,:]\n",
    "\n",
    "''' Define input/output dimensions of data. '''\n",
    "input_dim = train_y.shape[-1] # dimension of input data\n",
    "output_dim = train_v.shape[-1] # dimension of output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize active subspace, $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Initialize U0. '''\n",
    "if init_type == 'id':\n",
    "    U0 = np.eye(input_dim, acs_dim)\n",
    "elif init_type == 'rn':\n",
    "    U0 = np.linalg.qr(np.random.normal(size=(input_dim,acs_dim)))[0] # QR decomp, take Q for orthonomal random matrix\n",
    "elif init_type == 'acs':\n",
    "    U0, _, _ = np.linalg.svd(train_dv.transpose(), full_matrices=False)\n",
    "    U0 = U0[:,:acs_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the two layer ReLU network, $g$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Notation:\n",
    "        h_0 = first layer of network\n",
    "        h_last = last layer of network\n",
    "        h_i (1 <= i <= last-1) can refer to the ith hidden layer\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.h_0 = nn.Linear(input_dim, hidden_dim) \n",
    "        self.h_last = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.ReLU()\n",
    "        return\n",
    "    def forward(self, x0, U):\n",
    "        x0 = torch.matmul(x0, U)\n",
    "        x1 = self.h_0(x0)\n",
    "        x2 = self.sigmoid(x1)\n",
    "        x3 = self.h_last(x2)\n",
    "        return x3\n",
    "    \n",
    "''' Define network structure. '''\n",
    "net = MLP(input_dim=acs_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=l2_reg_scale) \n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Alternating Minimization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a collection of functions to carry out the alternating minimization algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Neural Network Approximation Subproblem, Related Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "source_hidden"
    ]
   },
   "outputs": [],
   "source": [
    "''' Neural Network Functions. '''\n",
    "\n",
    "def print_epoch(epoch,num_epoch,loss_train,loss_val,overwrite):\n",
    "    '''\n",
    "    NOTES: Structure for nice printing of train/validation loss for given epoch.\n",
    "\n",
    "    INPUT: \n",
    "        epoch = int; current epoch number\n",
    "        num_epoch = int; total number of epochs that will be executed\n",
    "        loss_train = float; error for training data\n",
    "        loss_val = float; error for validation data\n",
    "        overwrite = bool; choice to overwrite printed line\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "    assert (type(epoch) == int),'epoch must be int.'\n",
    "    assert (type(num_epoch) == int),'num_epoch must be int.'\n",
    "    assert (type(overwrite) == bool),'overwrite must be bool.'\n",
    "\n",
    "    line = 'Epoch {}/{}'.format(epoch+1, num_epoch)\n",
    "    line += ' | ' + 'Train Rel Loss: {:.8f}'.format(loss_train)\n",
    "    line += ' | ' + 'Validation Rel Loss: {:.8f}'.format(loss_val)\n",
    "    if overwrite:\n",
    "        print(line, end='\\r')\n",
    "    else:\n",
    "        print(line)\n",
    "\n",
    "def train_nnwsub(train_y,train_v,val_y,val_v,U,net,criterion,optimizer,batch_size,num_epoch,numPrint_epoch,epoch_sto,train_loss_sto,val_loss_sto,denom):\n",
    "    '''\n",
    "    NOTES: Trains neural network composed with the linear function U, i.e. net(U(train_y))\n",
    "            and checks against validation data, and saves network state_dict and optimizer state_dict. \n",
    "            All data has the following structure: axes\n",
    "                - 0 = ith sample\n",
    "                - -1 = ith dimension of data\n",
    "\n",
    "    INPUT: \n",
    "        train_y = training data input\n",
    "        train_v = training data output\n",
    "        val_y = validation data input\n",
    "        val_v = validation data output\n",
    "        U = 2D array; U.shape[0] = dimension of input data, U.shape[1] = desired dimension of subspace\n",
    "        net = network function\n",
    "        criterion = loss function\n",
    "        optimizer = optimization algorithm\n",
    "        train_loss_sto = array/list to store loss values\n",
    "        dir_name = string with name of directory to save results\n",
    "        args = user input/parameters\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "    assert (len(U.shape) == 2),'U must be 2D array'\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # convert data to torch tensor\n",
    "    train_y_tensor = torch.from_numpy(train_y).float()\n",
    "    train_v_tensor = torch.from_numpy(train_v).float()\n",
    "    val_y_tensor = torch.from_numpy(val_y).float()\n",
    "    val_v_tensor = torch.from_numpy(val_v).float()\n",
    "\n",
    "    U = torch.from_numpy(U).float()\n",
    "\n",
    "    # create batches for training data\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_y_tensor, train_v_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # run training\n",
    "    current_step = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, (train_y_batch, train_v_batch) in enumerate(train_loader):  \n",
    "            current_step += 1\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(net(train_y_batch,U), train_v_batch)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        if (epoch+1) % numPrint_epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                loss_train = criterion(net(train_y_tensor,U), train_v_tensor)\n",
    "                loss_val = criterion(net(val_y_tensor,U), val_v_tensor)\n",
    "                epoch_sto.append(epoch_sto[-1]+numPrint_epoch) # store new epoch value\n",
    "                train_loss_sto.append(np.sqrt(loss_train.numpy()/denom)) # store new loss value\n",
    "                val_loss_sto.append(np.sqrt(loss_val.numpy()/denom)) # store new loss value\n",
    "                print_epoch(epoch, num_epoch, train_loss_sto[-1], val_loss_sto[-1], overwrite=False) # print after print_epoch number of epochs\n",
    "                    \n",
    "    end = time.time()\n",
    "    print('\\nNetwork training running time: {}'.format(end-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Subspace Approximation Subproblem, Related Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Subspace code. '''\n",
    "def sub_res(train_y,train_v,U,net,criterion):\n",
    "    '''\n",
    "    NOTES: Defines cost function w.r.t. U. \n",
    "            All data has the following structure: axes\n",
    "                - 0 = ith sample\n",
    "                - -1 = ith dimension of data\n",
    "\n",
    "    INPUT: \n",
    "        train_y = training data input\n",
    "        train_v = training data output\n",
    "        U = 2D array; U.shape[0] = dimension of input data, U.shape[1] = desired dimension of subspace\n",
    "        net = network function\n",
    "        criterion = loss function\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "    assert (len(U.shape) == 2),'U must be 2D array'\n",
    "\n",
    "    # convert data to torch tensor\n",
    "    train_y = torch.from_numpy(train_y).float()\n",
    "    train_v = torch.from_numpy(train_v).float()\n",
    "\n",
    "    U = torch.from_numpy(U).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = criterion(net(train_y,U),train_v)\n",
    "\n",
    "    return out\n",
    "\n",
    "def sub_dres(train_y,train_v,U,net,criterion):\n",
    "    '''\n",
    "    NOTES: Defines derivative of cost function w.r.t. U. \n",
    "            All data has the following structure: axes\n",
    "                - 0 = ith sample\n",
    "                - -1 = ith dimension of data\n",
    "\n",
    "    INPUT: \n",
    "        train_y = training data input\n",
    "        train_v = training data output\n",
    "        U = 2D array; U.shape[0] = dimension of input data, U.shape[1] = desired dimension of subspace\n",
    "        net = network function\n",
    "        criterion = loss function\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "    assert (len(U.shape) == 2),'U must be 2D array'\n",
    "\n",
    "    # convert data to torch tensor\n",
    "    train_y = torch.from_numpy(train_y).float()\n",
    "    train_v = torch.from_numpy(train_v).float()\n",
    "\n",
    "    U = torch.from_numpy(U).float()\n",
    "\n",
    "    U.requires_grad = True\n",
    "\n",
    "    loss = criterion(net(train_y,U),train_v)\n",
    "    loss.backward()\n",
    "\n",
    "    return U.grad.numpy()\n",
    "\n",
    "def train_sub(train_y,train_v,U0,net,criterion,denom):\n",
    "    '''\n",
    "    NOTES: Trains subspace U. \n",
    "            All data has the following structure: axes\n",
    "                - 0 = ith sample\n",
    "                - -1 = ith dimension of data\n",
    "\n",
    "    INPUT: \n",
    "        train_y = training data input\n",
    "        train_v = training data output\n",
    "        U0 = 2D array; U0.shape[0] = dimension of input data, U0.shape[1] = desired dimension of subspace\n",
    "        net = network function\n",
    "        criterion = loss function\n",
    "\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "    assert (len(U0.shape) == 2),'U0 must be 2D array'\n",
    "\n",
    "    # instantiate the Grassmann manifold  \n",
    "    m, n = U0.shape\n",
    "    manifold = Grassmann(m, n)\n",
    "\n",
    "    # print loss before optimization (sanity check)\n",
    "    print('\\nLoss before Grassman Opt: {:.8f}'.format(np.sqrt(sub_res(train_y,train_v,U0,net,criterion)/denom)))\n",
    "\n",
    "    # define cost and gradient functions\n",
    "    cost = lambda U: sub_res(train_y, train_v, U, net, criterion)\n",
    "    grad = lambda U: sub_dres(train_y, train_v, U, net, criterion)\n",
    "\n",
    "    # instantiate optimization problem over Grassman manifold\n",
    "    problem = Problem(manifold=manifold, cost=cost, egrad=grad, verbosity=0)\n",
    "    solver = SteepestDescent(logverbosity=0)\n",
    "\n",
    "    U = solver.solve(problem, x=U0) # update U\n",
    "\n",
    "    # print loss after optimization\n",
    "    loss_train = sub_res(train_y,train_v,U,net,criterion)\n",
    "    print('Loss after Grassman Opt: {:.8f}'.format(np.sqrt(loss_train/denom)))\n",
    "\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin alternating minimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alternating minimization iteration count: 1\n",
      "\n",
      "Epoch 500/5000 | Train Rel Loss: 0.23187724 | Validation Rel Loss: 0.21485305\n",
      "Epoch 1000/5000 | Train Rel Loss: 0.16013407 | Validation Rel Loss: 0.17038971\n",
      "Epoch 1500/5000 | Train Rel Loss: 0.16228851 | Validation Rel Loss: 0.17232800\n",
      "Epoch 2000/5000 | Train Rel Loss: 0.17117875 | Validation Rel Loss: 0.17957860\n",
      "Epoch 2500/5000 | Train Rel Loss: 0.15730448 | Validation Rel Loss: 0.17055952\n",
      "Epoch 3000/5000 | Train Rel Loss: 0.15659562 | Validation Rel Loss: 0.16910946\n",
      "Epoch 3500/5000 | Train Rel Loss: 0.17648117 | Validation Rel Loss: 0.18543608\n",
      "Epoch 4000/5000 | Train Rel Loss: 0.16874821 | Validation Rel Loss: 0.17803754\n",
      "Epoch 4500/5000 | Train Rel Loss: 0.15597356 | Validation Rel Loss: 0.16841686\n",
      "Epoch 5000/5000 | Train Rel Loss: 0.18850379 | Validation Rel Loss: 0.20230684\n",
      "\n",
      "Network training running time: 11.31084156036377\n",
      "\n",
      "Loss before Grassman Opt: 0.18850379\n",
      "Loss after Grassman Opt: 0.15788774\n",
      "\n",
      "Alternating minimization iteration count: 2\n",
      "\n",
      "Epoch 500/5000 | Train Rel Loss: 0.13033014 | Validation Rel Loss: 0.22007358\n",
      "Epoch 1000/5000 | Train Rel Loss: 0.13003653 | Validation Rel Loss: 0.22204493\n",
      "Epoch 1500/5000 | Train Rel Loss: 0.14937506 | Validation Rel Loss: 0.22438046\n",
      "Epoch 2000/5000 | Train Rel Loss: 0.13176025 | Validation Rel Loss: 0.22425419\n",
      "Epoch 2500/5000 | Train Rel Loss: 0.15903900 | Validation Rel Loss: 0.24804279\n",
      "Epoch 3000/5000 | Train Rel Loss: 0.13009707 | Validation Rel Loss: 0.22173242\n",
      "Epoch 3500/5000 | Train Rel Loss: 0.13793136 | Validation Rel Loss: 0.22071412\n",
      "Epoch 4000/5000 | Train Rel Loss: 0.13335635 | Validation Rel Loss: 0.22605331\n",
      "Epoch 4500/5000 | Train Rel Loss: 0.13796051 | Validation Rel Loss: 0.22010224\n",
      "Epoch 5000/5000 | Train Rel Loss: 0.12992796 | Validation Rel Loss: 0.22055387\n",
      "\n",
      "Network training running time: 11.210870265960693\n",
      "\n",
      "Loss before Grassman Opt: 0.12992796\n",
      "Loss after Grassman Opt: 0.12349320\n",
      "\n",
      "Alternating minimization iteration count: 3\n",
      "\n",
      "Epoch 500/5000 | Train Rel Loss: 0.12243001 | Validation Rel Loss: 0.21994059\n",
      "Epoch 1000/5000 | Train Rel Loss: 0.12293928 | Validation Rel Loss: 0.22178503\n",
      "Epoch 1500/5000 | Train Rel Loss: 0.12339006 | Validation Rel Loss: 0.22188527\n",
      "Epoch 2000/5000 | Train Rel Loss: 0.13200937 | Validation Rel Loss: 0.22717638\n",
      "Epoch 2500/5000 | Train Rel Loss: 0.14050830 | Validation Rel Loss: 0.23315814\n",
      "Epoch 3000/5000 | Train Rel Loss: 0.12240239 | Validation Rel Loss: 0.22101575\n",
      "Epoch 3500/5000 | Train Rel Loss: 0.12284839 | Validation Rel Loss: 0.22051850\n",
      "Epoch 4000/5000 | Train Rel Loss: 0.12212248 | Validation Rel Loss: 0.22091141\n",
      "Epoch 4500/5000 | Train Rel Loss: 0.12335429 | Validation Rel Loss: 0.22051973\n",
      "Epoch 5000/5000 | Train Rel Loss: 0.13595813 | Validation Rel Loss: 0.22993992\n",
      "\n",
      "Network training running time: 11.183346271514893\n",
      "\n",
      "Loss before Grassman Opt: 0.13595814\n",
      "Loss after Grassman Opt: 0.13457075\n",
      "\n",
      "Alternating minimization iteration count: 4\n",
      "\n",
      "Epoch 500/5000 | Train Rel Loss: 0.13423614 | Validation Rel Loss: 0.23296914\n",
      "Epoch 1000/5000 | Train Rel Loss: 0.12779687 | Validation Rel Loss: 0.22753439\n",
      "Epoch 1500/5000 | Train Rel Loss: 0.12508635 | Validation Rel Loss: 0.22437214\n",
      "Epoch 2000/5000 | Train Rel Loss: 0.12477255 | Validation Rel Loss: 0.22434911\n",
      "Epoch 2500/5000 | Train Rel Loss: 0.12577005 | Validation Rel Loss: 0.22621303\n",
      "Epoch 3000/5000 | Train Rel Loss: 0.12458892 | Validation Rel Loss: 0.22377208\n",
      "Epoch 3500/5000 | Train Rel Loss: 0.12486606 | Validation Rel Loss: 0.22252794\n",
      "Epoch 4000/5000 | Train Rel Loss: 0.14828138 | Validation Rel Loss: 0.24332636\n",
      "Epoch 4500/5000 | Train Rel Loss: 0.12798202 | Validation Rel Loss: 0.22769063\n",
      "Epoch 5000/5000 | Train Rel Loss: 0.12408547 | Validation Rel Loss: 0.22355193\n",
      "\n",
      "Network training running time: 11.397831678390503\n",
      "\n",
      "Loss before Grassman Opt: 0.12408546\n",
      "Loss after Grassman Opt: 0.12189969\n",
      "\n",
      "Alternating minimization iteration count: 5\n",
      "\n",
      "Epoch 500/5000 | Train Rel Loss: 0.12208839 | Validation Rel Loss: 0.22467928\n",
      "Epoch 1000/5000 | Train Rel Loss: 0.12356178 | Validation Rel Loss: 0.22614555\n",
      "Epoch 1500/5000 | Train Rel Loss: 0.12275954 | Validation Rel Loss: 0.22555669\n",
      "Epoch 2000/5000 | Train Rel Loss: 0.12141299 | Validation Rel Loss: 0.22399064\n",
      "Epoch 2500/5000 | Train Rel Loss: 0.13598188 | Validation Rel Loss: 0.22971515\n",
      "Epoch 3000/5000 | Train Rel Loss: 0.12338493 | Validation Rel Loss: 0.22585498\n",
      "Epoch 3500/5000 | Train Rel Loss: 0.12312391 | Validation Rel Loss: 0.22423917\n",
      "Epoch 4000/5000 | Train Rel Loss: 0.12209130 | Validation Rel Loss: 0.22351722\n",
      "Epoch 4500/5000 | Train Rel Loss: 0.12480101 | Validation Rel Loss: 0.22761255\n",
      "Epoch 5000/5000 | Train Rel Loss: 0.12226034 | Validation Rel Loss: 0.22513346\n",
      "\n",
      "Network training running time: 11.451116561889648\n",
      "\n",
      "Loss before Grassman Opt: 0.12226034\n",
      "Loss after Grassman Opt: 0.12216820\n"
     ]
    }
   ],
   "source": [
    "ite = 0 # initialize outer iteration counter\n",
    "U = np.copy(U0) # to not overwrite U0, if wanted for reference later\n",
    "\n",
    "denom = np.linalg.norm(v,ord='fro')**2/v.shape[0] # denominator for relative error\n",
    "epoch_sto = [0] # initialize list to store epoch values where loss is calculate/stored\n",
    "with torch.no_grad():\n",
    "    train_loss_sto = [np.sqrt(criterion(net(torch.from_numpy(train_y).float(),torch.from_numpy(U).float()), torch.from_numpy(train_v).float())/denom)] # initialize list to store training loss\n",
    "    val_loss_sto = [np.sqrt(criterion(net(torch.from_numpy(val_y).float(),torch.from_numpy(U).float()), torch.from_numpy(val_v).float())/denom)] # initialize list to store validation loss\n",
    "\n",
    "''' Begin Alternating Minimization Process. '''\n",
    "while ite < max_outIter:\n",
    "    ''' Update iteration count. '''\n",
    "    ite += 1\n",
    "    print('\\nAlternating minimization iteration count: {}\\n'.format(ite))\n",
    "\n",
    "    ''' Train network. '''\n",
    "    train_nnwsub(train_y,train_v,val_y,val_v,U,\n",
    "                 net,criterion,optimizer,batch_size,num_epoch,numPrint_epoch,\n",
    "                 epoch_sto,train_loss_sto,val_loss_sto,denom)    \n",
    "    optimizer = torch.optim.Adam(\n",
    "                    net.parameters(), \n",
    "                    lr=(lr_decay**(ite))*lr,\n",
    "                    weight_decay=l2_reg_scale\n",
    "                    ) \n",
    "    \n",
    "    ''' Train subspace. '''\n",
    "    U = train_sub(train_y,train_v,U,net,criterion,denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relative error, defined below.\n",
    "$$\n",
    "\\text{RelError}_{p,n}  = \\left( \\frac{\\frac{1}{| X_{Validation} |} \\sum_{x\\in X_{Validation}} \\| f(x)-g_{\\theta^{p,n}}((U^{p,0})^T x)\\|_2^2}{\\frac{1}{|X|} \\sum_{x\\in X} \\| f(x) \\|_2^2} \\right)^{\\frac{1}{2}} \n",
    ".\n",
    "$$\n",
    "(Where, as defined in <a name=\"ref-1\"/>[(Bollinger and Schaeffer, 2020)](#cite-bollinger2020reduced), $X_{Validation}$ is the validation set, and $X$ is the entire avaliable data set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEWCAYAAABFSLFOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA0x0lEQVR4nO3dd3wc9bX38c9RXa17xcYGbBNiWogdjB8IzaFdA6ElEAyhXMKlhM5DnmCSmwA35FLCTQhcAqElJPTeAg7NpptiYozpGGwsG3dLlqwuneeP3y5ai5W0kna1kvb7fr3mtbuzU87sSnP2/GbmN+buiIiIpEtetgMQEZG+RYlFRETSSolFRETSSolFRETSSolFRETSSolFRETSSolFpJczs2lmVprtOETilFhE2mBmi81spZn1Sxj3H2Y2p8V0Zmafmdn7SZZhZnaOmS00s41mVmpm95vZt1pMd4mZuZlNzdgGiXQDJRaR9hUA57YzzV7ASGCCme3S4r0/xuY/BxgKfBN4BDg4PoGZGXA8sA44MS1Rh+UWpGtZIqlSYhFp3++An5nZ4DamORF4FHiShMRgZtsAZwLHuPvz7l7r7lXufqe7X5Ew/57A5oQENMPMilpbkZmVmNlfzWx9rELapcX7i83sQjNbAGw0swIzm2lmi8yswszeN7MjEqbPN7P/MbM1Zva5mZ0Vq5yUlKRTlFhE2vcWMAf4WbI3zSwKHAncGRsSE8O+QKm7v9HOOk4EHgfujb3+fhvTXgxsHRv+jeQVzjGEimiwuzcAiwjJaxBwKXCHmY2OTXsKcCAwCfgOcHg7sYq0SYlFJDW/Bs42sxFJ3vsBUAs8DTxBaDqLN3MNA75sa8GxxHQUcJe71wMP0HZz2I+A37r7OndfClybZJpr3X2pu1cDuPv97r7c3Zvc/V7gE2BqwvL+6O6l7r4euCLJ8kRSpsQikgJ3X0hIGjOTvH0icJ+7N7h7LfAQzYlhLTA6yTyJjgAaCM1oEKqeA1tJYhCazJYmvF6SZJrE9zGzE8xsvpmVmVkZsCMwvJXlbTKvSEcpsYik7mJCs9GY+AgzGwvsAxxnZivMbAWhWewgMxsOPAeMNbMpbSz3RKA/8EVs/vuBQkJzVjJfAlskvN4yyTRfdVtuZlsBNwNnAcPcfTCwELCE5Y1NmDdx2SIdpsQikiJ3/5RwDOSchNHHAx8DEwnHKCYRzvoqJRyw/wT4E3B37HqTIjOLmNmM2AH1MYTjMN9PmP/bwJW03hx2H3CRmQ2JJbaz2wm9HyHRrAYws5MIFUvi8s41szGxExQubGd5Im1SYhHpmP8i7KjjTgT+5O4rEgfgRpoTwznA/wLXA2WEA+lHEA7WHw/Md/enW8x/LbCTmSUmgLhLCc1fnxOO6/y9rYDd/X3gf4DXgJXAt4BXEia5ObacBcC/CE1yDUBj+x+HyNeZbvQlIonM7EDgRnffKtuxSO+kikUkx8Wuizkodr3LGMKxpIezHZf0XqpYRHJc7HTnF4BtgWrgH8C57r4hq4FJr6XEIiIiaaWmMBERSauc7wto+PDhPm7cuGyHISLSq8ybN2+Nuye9iDfnE8u4ceN46623sh2GiEivYmbJenwA1BQmIiJppsQiIiJppcQiIiJplfPHWEREOqq+vp7S0lJqamqyHUrGRSIRxo4dS2FhYcrzKLGIiHRQaWkpAwYMYNy4cYS7SvdN7s7atWspLS1l/PjxKc+npjARkQ6qqalh2LBhfTqpAJgZw4YN63BlpsQiItIJfT2pxHVmO5VYOumZ377BI2c+k+0wRER6HCWWThpw4+/Y8ZZzsx2GiOSosrIy/vSnP3V4voMOOoiysrL0B5RAiaWTGotLKGqsznYYIpKjWkssjY1t35/tySefZPDgwRmKKtBZYZ3UVBwl0lSV7TBEJEfNnDmTRYsWMWnSJAoLC+nfvz+jR49m/vz5vP/++xx++OEsXbqUmpoazj33XE499VSguRuryspKDjzwQPbYYw9effVVxowZw6OPPkpJSUmXY1Ni6SQviVLiSiwiue6882D+/PQuc9IkuOaatqe54oorWLhwIfPnz2fOnDkcfPDBLFy48KvTgm+77TaGDh1KdXU1u+yyCz/84Q8ZNmzYJsv45JNPuPvuu7n55pv50Y9+xIMPPshxxx3X5fiVWDrJIyWUUI03OZaXG2eHiEjPNXXq1E2uNbn22mt5+OFwI9ClS5fyySeffC2xjB8/nkmTJgGw8847s3jx4rTEosTSWf2iFNBIdUU9JYOKsh2NiGRJe5VFd+nXr99Xz+fMmcOzzz7La6+9RjQaZdq0aUmvRSkuLv7qeX5+PtXV6TlurIP3nZQXDe2Q1et0AF9Eut+AAQOoqKhI+l55eTlDhgwhGo3y4YcfMnfu3G6NLWcrFjM7BDjkG9/4Rufm7xcFoHptFYwflMbIRETaN2zYMHbffXd23HFHSkpK2Gyzzb56b/r06dx4443stNNOTJw4kV133bVbY8vZxOLujwOPT5ky5ZTOzJ8/ICSW2jJVLCKSHXfddVfS8cXFxTz11FNJ34sfRxk+fDgLFy78avzPfvaztMWlprBOyu8fmsLqynRmmIhIIiWWTioYGCoWJRYRkU0psXRSwYBYxVKupjARkURKLJ1UNDhULA0bVLGIiCRSYumkokGhYmmoUMUiIpJIiaWTioeEiqWxQhWLiEgiJZZOigwNiaWpUolFRHq+/v37d9u6lFg6KTIkNIU1VakpTEQkUc5eINlVJcNCxcJGVSwi0v0uvPBCttpqK8444wwALrnkEsyMF198kfXr11NfX89ll13GYYcd1u2xKbF0Ul5JMU0YropFJLdlqd/8GTNmcN55532VWO677z5mzZrF+eefz8CBA1mzZg277rorhx56aKfuW98VSiydZUY1JVi1KhYR6X6TJ09m1apVLF++nNWrVzNkyBBGjx7N+eefz4svvkheXh7Lli1j5cqVjBo1qltjU2Lpgpq8KFajikUkp2Wx3/wjjzySBx54gBUrVjBjxgzuvPNOVq9ezbx58ygsLGTcuHFJu8vPNCWWLqjNKyG/VhWLiGTHjBkzOOWUU1izZg0vvPAC9913HyNHjqSwsJDZs2ezZMmSrMSlxNIFtflRJRYRyZoddtiBiooKxowZw+jRo/nxj3/MIYccwpQpU5g0aRLbbrttVuJSYumC+vwS8uvUFCYi2fPuu+9+9Xz48OG89tprSaerrKzsrpB0HUtX1BVGKaxXxSIikkiJpQsaCksoaFDFIiKSSImlCxqKohQ1qGIRyUXunu0QukVntlOJpQsai6IUNapiEck1kUiEtWvX9vnk4u6sXbuWSCTSofl08L4LmiIlRJpUsYjkmrFjx1JaWsrq1auzHUrGRSIRxo4d26F5lFi6wCNRJRaRHFRYWMj48eOzHUaPpaawLvBICSVU08erYRGRDumTicXMJpjZrWb2QEZXFI3SjypqqpVZRETiMp5YzCzfzP5lZk90YRm3mdkqM1uY5L3pZvaRmX1qZjMB3P0zdz+5K3GnFFc03JOlan1tplclItJrdEfFci7wQbI3zGykmQ1oMe4bSSb9KzA9yfz5wPXAgcD2wDFmtn1XA06V9Qv3ZKlZp+MsIiJxGU0sZjYWOBi4pZVJ9gYeNbNIbPpTgGtbTuTuLwLrksw/Ffg0VqHUAfcAKd3VxswOMbObysvLU5k8qbz+IbHUlumUYxGRuExXLNcAPweakr3p7vcDs4B7zOzHwE+AH3Vg+WOApQmvS4ExZjbMzG4EJpvZRa2s+3F3P3XQoEEdWN2m8vuHpjBVLCIizTJ2urGZfR9Y5e7zzGxaa9O5+1Vmdg9wA7C1u3ekp7Rkt0Vzd18LnN6ReDujYGCoWOrKlFhEROIyWbHsDhxqZosJTVT7mNkdLScysz2BHYGHgYs7uI5SYIuE12OB5Z2KthMKB4aKpa5cTWEiInEZSyzufpG7j3X3ccAM4Hl3Py5xGjObDNxMOC5yEjDUzC7rwGreBLYxs/FmVhRbz2Np2YAUxCuWhg2qWERE4rJ9HUsUOMrdF7l7E3Ai8LVbnpnZ3cBrwEQzKzWzkwHcvQE4C/gn4cyz+9z9ve4KvmhwSCz1G1SxiIjEdUuXLu4+B5iTZPwrLV7XEyqYltMd08aynwSe7HKQnVA8ODSFNVaoYhERict2xdKrFQ8JFUtTpRKLiEicEksXlAwNFUvTRjWFiYjEKbF0QWRoqFh8oyoWEZE4JZYuyOsXKhavVsUiIhKnxNIVhYU0kI9VqWIREYlTYukKM6otitWoYhERiVNi6aLavBLyalWxiIjEKbF0UW1+lPwaJRYRkTglli6qyy8hv05NYSIicUosXVRXEKWgXhWLiEicEksX1RdGKaxXxSIiEqfE0kWNhSUUNqhiERGJU2LpoobiKEWNqlhEROKUWLqoqbiESKMqFhGROCWWLmqKRIm4EouISJwSS1dFSoh4Ne7ZDkREpGdQYukij0aJUkVNTbYjERHpGZRYuqokSoRaqiqbsh2JiEiPoMTSRfGu86vX6cwwERFQYumyvP7hZl+163UAX0QElFi6LL9/qFhq1qtiEREBJZYuyx8QKpa6MlUsIiKgxNJlBQNjiaVcFYuICCixdFnhwNAUVl+uikVEBJRYuqxwUKhY6jeoYhERgT6aWMxsgpndamYPZHpdxYNDxdJYoYpFRAQymFjMLGJmb5jZO2b2npld2oVl3WZmq8xsYZL3ppvZR2b2qZnNBHD3z9z95K7En6riIaFiUWIREQnaTCxmlpdsZ56iWmAfd/82MAmYbma7tlj+SDMb0GLcN5Is66/A9CTx5QPXAwcC2wPHmNn2nYy3UyJDQsXStFFNYSIi0E5icfcm4B0z27KjC/agMvayMDa07Kpxb+BRM4sAmNkpwLVJlvUisC7JaqYCn8YqlDrgHuCwVOIzs0PM7Kby8vKUtqc1kaGhYvGNqlhERCC1prDRwHtm9pyZPRYfUlm4meWb2XxgFfCMu7+e+L673w/MAu4xsx8DPwF+1IH4xwBLE16XAmPMbJiZ3QhMNrOLks3o7o+7+6mDBg3qwOq+Ln7lvVepYhERAShIYZpOHxtx90ZgkpkNBh42sx3dfWGLaa4ys3uAG4CtE6qcVFjy1fpa4PTOxt0hJaEpjCpVLCIikELF4u4vAB8CA2LDB7FxKXP3MmAOyY+T7AnsCDwMXNyR5RIqlC0SXo8FlndwGV2Tn08tRVCjikVEBFJILGb2I+AN4ChCM9XrZnZkCvONiFUqmFkJsB8hQSVOMxm4mXBc5CRgqJld1oH43wS2MbPxZlYEzABSaqZLp9q8EvJrVLGIiEBqTWG/BHZx91UQEgbwLNDeNSKjgdtjZ27lAfe5+xMtpokCR7n7otiyTwT+veWCzOxuYBow3MxKgYvd/VZ3bzCzs4B/AvnAbe7+XgrblFY1eVHyapVYREQgtcSSF08qMWtJrQltATC5nWleafG6nlDBtJzumDaW8STwZHvxZFJdfpT8WjWFiYhAaolllpn9E7g79vposrwj72nqC0ooqFPFIiIC7SQWMzPCdSW7AHsQzsK6yd0f7obYeo26wigF9apYRESgncTi7m5mj7j7zsBD3RRTr9NQWEKRjrGIiACpXSA518x2yXgkvVhjUZSiRiUWERFI7RjL94DTzGwJsJHQHObuvlNGI+tFGotLiDSqKUxEBFI7xnI6sKR7wumdmiJRIk2qWEREILVjLH+IHWORVngkSsSrcQdL1smMiEgO0TGWdCgpIUoVNTXZDkREJPtSPcZyupktRsdYkotGKaGaiqrmPilFRHJVKonlwIxH0dtFSyiinqryeoYNK8x2NCIiWZVK1yxLCD0I7xN7XpXKfLkkr1+4J0ttmc4MExFJpXfji4ELgfgNswqBOzIZVG+TNyAklpr1SiwiIqlUHkcAhxKOr+Duywn3ZZGYgv7hwErtep1yLCKSSmKpc3cndr96M+uX2ZB6n4KBoWKpK1fFIiKSSmK5z8z+DAw2s1MI92L5Wtf2uaxwYKhY6spUsYiItHtWmLtfbWb7AxuAicCv3f2ZjEfWixQOChVLQ4UqFhGRVE43JpZIlExaUTQoVCwNFapYRER02nAaFA0OFUujEouIiBJLOkSGhsTSVKmmMBGRlBKLmZWY2cRMB9NbRYaEprCmSlUsIiKpXCB5CDAfmBV7PcnMHstwXL1KXv9QsXiVKhYRkVQqlkuAqUAZgLvPB8ZlKqBeKd7zZJUqFhGRVBJLg7uXZzyS3iwSAcCqlVhERFI53XihmR0L5JvZNsA5wKuZDauXycuj2kqwGjWFiYikUrGcDewA1AJ3AeXAeRmMqVeqzSshr1YVi4hIKhXLRHf/JfDLTAfTm9XmR8mvVcUiIpJKxfJ7M/vQzH5jZjtkPKJeqj6/hII6VSwiIqnc6Ot7wDRgNXCTmb1rZv+Z6cB6m7rCKPn1qlhERFK6QNLdV7j7tcDphGtafp3JoHqjhsISiupVsYiIpHKB5HZmdomZLQT+l3BG2NiMR9bLNBRFKWpUYhERSeXg/V+Au4EDYnePlCQai6IUNZRlOwwRkaxL5X4su3ZHIL1dU3EJxU2qWEREWk0sZnafu//IzN4ldlvi+FuAu/tOGY+uF2kqiRLxatzBLNvRiIhkT1sVy7mxx+93RyC9XqSEKFXU1DR3HSYikotaPXjv7l/Gnp7h7ksSB+CM7gmvF4lGKaFa/VCKSM5L5XTj/ZOMOzDdgfR6/aJEqaK6ytufVkSkD2vrGMtPCZXJBDNbkPDWAOCVTAfW21i0hHyaqC6vgy2Ksx2OiEjWtHWM5S7gKeByYGbC+Ap3X5fRqHqh/NjNvmrWVwNKLCKSu1pNLLF7sJQDxwCY2UggAvQ3s/7u/kX3hNg75PcPR+xr1lUBg7Mai4hINqV0a2Iz+wT4HHgBWEyoZHosM5tgZrea2QPdtc78AaFiqStXf2EikttSOXh/GbAr8LG7jwf2JYVjLGa2hZnNNrMPzOw9Mzu3vXnaWNZtZrYq1q1My/emm9lHZvapmc0EcPfP3P3kzq6vMwoHhoqlrkynhYlIbkslsdS7+1ogz8zy3H02MCmF+RqAC9x9O0JiOtPMtk+cwMxGmtmAFuO+kWRZfwWmtxxpZvnA9YSz1LYHjmm5ju5SOChULPUbVLGISG5LJbGUmVl/4EXgTjP7IyFptMndv3T3t2PPK4APgDEtJtsbeNTMIgBmdgpwbZJlvQgkO2FgKvBprEKpA+4BDkthm9KuaHBILI0VqlhEJLelklgOA6qB84FZwCLgkI6sxMzGAZOB1xPHu/v9sWXeY2Y/Bn4C/KgDix4DLE14XQqMMbNhZnYjMNnMLmolpkPM7Kby8vIOrK51RYNCU5gSi4jkulQ6odyY8PL2jq4gVu08CJzn7huSLP8qM7sHuAHY2t0rO7L4JOM81nR3elszuvvjwONTpkw5pQPra1XxkFjFUqmmMBHJba1WLGZWYWYbEoaKxMdUFm5mhYSkcqe7P9TKNHsCOwIPAxd3MP5SYIuE12OBrHTtHxkSKpamSlUsIpLb2uorbIC7D0wYBiQ+trdgMzPgVuADd/99K9NMBm4mNLedBAw1s8s6EP+bwDZmNt7MioAZwGMdmD9t8mIXSDZtVMUiIrktpVsTm9keZnZS7PlwMxufwmy7A8cD+5jZ/NhwUItposBR7r7I3ZuAE4ElSdZ/N/AaMNHMSs3sZAB3bwDOAv5JODngPnd/L5VtSrtoSCzqhVJEcl27x1jM7GJgCjCRcDfJIuAOQuJolbu/TPJjIInTvNLidT2hgmk53TFtLONJ4Mm21tMtYn3lW7USi4jktlQqliOAQ4GNALHbEw9oc45cVFREI3lYjZrCRCS3pZJY6tzdid1F0sz6ZTakXsqM2rwS8mpUsYhIbkslsdxnZn8GBscuYHyWJM1VArV5UfJqVbGISG5r8xhL7Myue4FtgQ2E4yy/dvdnuiG2XqeuoIT8OlUsIpLb2kws7u5m9oi77wwombSjriBKQZ0qFhHJbak0hc01s10yHkkfUF8YpbBeFYuI5LZ2TzcGvgecZmZLCGeGGaGY2SmjkfVCjYUlFOk6FhHJcakklgMzHkUf0VAUpbCxI12diYj0Pal0Qvm1K+EluabiEiKNq7IdhohIVqXUpYukpikSpdircc92JCIi2aPEkkYeKSFKFTU12Y5ERCR7lFjSKRqlhGr1QykiOU2JJZ2iUaJUUa1LWUQkhymxpJH1KyFKNVUbdZBFRHKXEksa5fUL92SpXq+DLCKSu5RY0ii/f7gnS+16HWQRkdylxJJG+QNCxVJbpoMsIpK7lFjSqGBgSCx1ZapYRCR3KbGkUeHA0BTWsEGJRURylxJLGhUNDhVL/QY1hYlI7lJiSaOiQaFiaaxQxSIiuUuJJY2Kh4SKpaFCFYuI5C4lljQqHhwqlqZKVSwikruUWNIor3+oWJqqVLGISO5SYkmnaEgsbFTFIiK5S4klnUpCU5hVK7GISO5SYkmnWGJR98YiksuUWNKpsJB6CsirUcUiIrlLiSXNavOj5NWqYkmrykr4xz+gqSnbkYhICpRY0qw2P0p+rSqWtKmogAMOgO9/Hy66KNvRSG/Q2Agff5ztKHKaEkua1ReUkF+niiUtKirgwAPhjTdCcrnqKrjuumxHJT1ZdTUcdRRMnAjHHANr1mQ7opykxJJm9QVRChtUsXRZZSUcfDDMnQv33ANPPgmHHw7nngsPPpjt6KQnWrcO9t8fHnkEZswIfyc77AAPPZTtyHJOQbYD6Gsaikoo0pX3XbNxY2j6evVVuOsuOPLIMP6uu2C//eDHP4aRI2HPPbMbZyY0NsK//gXPPQdlZXDmmTB2bHZiqa6GxYvhs8/CsGRJiGWPPWDSJCjoQbuPL76A6dNh0SK4995Qtbz7Lvz7v8MPfwhHHx2q3REjsh1pTjD33L4/+5QpU/ytt95K2/I+Hbs3y77MY/fa2T3q/67XqKoKSeWFF+DOO8Mvz0Rr18Luu8PKlfDKK7D99tmJM13c4cMP4fnnQzKZMwfWrw/v5edDYSGcfTbMnAlDh2Y2lrq68Cv/ttvgvffgyy83fb+oKEwD0K8f7LZbSO577AGTJ8OQIZmNrzXvvhuaTCsq4NFHYdq05vfq60MT6qWXwuDBIbkcdhhEItmJNe6LL2D27PB919SEymqHHWDHHWHChPDdp0N1NSxbBqNGQf/+6VlmjJnNc/cpSd9TYklvYlkxaTpL3llP4yuv893vpm2xfUNdXdhZLVsWdp5mkJe36ePll4d/tr//HY49NvlyFi8OO7XCQnjtNRgzpu31VlXB66/DSy/BvHmwzTah8tlrr+beErpLQwPMnw8vv9w8rFwZ3hs3DvbdF/bZJww1NfDrX8Mdd8CgQSG5nH12+mNetgz+/Ge46aYQy9Zbh89mwgQYP775cbPNwrTxuF96KezU4/uQoUPhG9/YdBg6NHyvLVVVwerVYVi1qvl5Y2P4PseO3XQYNSp8Bv37b7rTfeGFkCj69YOnnoKddkq+jQsXhupl3rww/8SJ8O1vNw877BC2o6Li60NZWRjWr28eysrCdhUXh6GoqPn5wIFhu4cMCY/x54sWNSeTzz4LcQ0bFqb//PPmWCMR2Hbb8D0MGxaGoUObHwcNCtfMRSLNj5FI+HtZsCAM77wThk8+aT6bctCg8FlusUXz53ryyZ2uiJVY2pDuxFL3/R/w4T8+5eFLFnDxxWlbbM9WXx/+aVauDMOqVc3P44lk2bIwvj1m8Le/wXHHtT3d/Plh57fFFmHH0r9/2Ln07x+GvDx46y148cWwM6mvD8v+5jfDP3FdXdgZfPe7Icnstx8MHx7G19dvOuTnN+80Eof4zjFxx7hqVWjKc//6sGRJSIQbN4ZtGD8+/Nrfa6+QSCZMSL6tCxbAL34RTrnefHP4v/83PBYVheRaVBSGgoKwY66vDwks/tjYGLahoCBMH3+srITbbw/HJJqawjGtM88MJ0rkpXj4tawsbNP778OnnzYPX3yR+unhgweHps0RI8J6ly2D0tLm6qilAQPCznjgwPB3t/XWMGsWbLll2+tpaIDHH4e3327e8X7xRWoxQljf4MEhSQweHMbV1UFtbRjizzdsCJ9LMkOGwN57w/e+FyqrHXcM21xZCR98ECrFhQvDsHhxSGJr14bvsCPGjw8Jc6edwg+WVavCZ5o4rFgRfhjsuGPHlh2jxNKGdCcWjjuOpQ/M5Zgpn/Lyy+lbbI/1+edwyCHhHyJRXl7YUYwaFX6BthzizTruYQcUfxw1KlQUqXj22fArdOXKsNNoqbAQdtklNNfstVdIIoMHh4Tw8sth/meeCUkqXSKR5sRmtukwYkRIJPHmo/YqrZZeegkuvDDsyNNl6FD4j/+A008PO6N0qa0NO8by8uTvRyLh8xg+PHxPLbmHHWriTnDDhk2H8vLwK/yqqzrfTLhuXUjcH34Y4hgwIHx/AwY0D4MHh/V0pG27sTEkl3XrmodRo8KOvqPNXPFKau3asJzy8lCdtBzy80OS+Na3QhJsT11dmKeTzW5KLG1Ie2I55RQ23PMPhlYvZ9261L7fXuuVV8KZWo2NcMUV4ZfjyJGhyWTYsPS1E6eiri786qusDBVBTU1oToh3s9OW1atDZbNxY3MFUFjYPDQ2Nv8qTRwikeZf2vHHfv2SN/2ki3v4lV1TE7Y5PsSrq4KCr1cm+flhGxKrmPr6sLypU1P7jERaaCux6PByum2zDQMrv2QbPmDOnO049NBsB5Qhd9wR2me32gqeeCI0MWVTUVFze3ZHjRgRzhzqDczCZy7Sg+k6lnQ76SS8uJjzC67jmWeyHUwGNDXBf/4nHH98aFqaOzf7SUVEehQllnQbMQI79lhO8NuZO6ss29GkV1VVuB7gt78N1co//5n5U2BFpNdRYsmEs88m0ljFnp/extKl2Q4mTdzhiCPCdQ5XXw033xyan0REWlBiyYTJk9n4nT05m+t49p8dPE2wp/rHP+Dpp+H3v4cLLsjsAWoR6dWUWDIkOvMcxrOYtbc/ke1Quq6hIZzmus024ToHEZE2KLFkiB1xOGujY5n6+rW9/zYit98eLoC7/PLk1xyIiCRQYsmUggKWHHwme9U/z0cPLsx2NJ1XVRW6Fdl1V24r+wGPPJLtgESkp1NiyaDNLz6FaiLUXp3Fe4hUVMD554d+oCorOz7/NdfA8uUsO/cqTjvdmDkz7RGKSB+jxJJBo3YYxj+GHMe2b/09dMXQ3dasCZ0aXnMNnHZa6F/qzDND/0Cpzn/llXDooVz4xJ40NMBHH3WseyURyT1KLBn2+cFnE2mqpv6GW1KfqaoKbr0VTjop9IbaGUuXhj6p3n0XHnusufuVW28N/RXtsUfolr62tvVlXHYZVFby8UmXc9ddoUswoG9e+CkiaaPEkmE7HrsTs5lGw7XXJ+8oMdFHH4VmqzFjQseA99wTer3db79whXuqPvoo3LNk+XLqHv8nF716CHd+/l3qb/1b6Dn26qtDx43HHRfuZ3L//c1dn8d99hn86U/wk5/ws9u2Z+BA+OtfYfRoJRYRaYe75/Sw8847eyZVVrofWfBQ6Dj9wQc3fbOhwb201P2BB9z33TdMU1jofswx7i+95F5V5f6HP7iPGBHeO/hg97ffbnuFb73lPny4+8iR3jTvbT/ppOZ+28eOdb/qKvf16929sdH9ySfdv/Wt8OZuu7m/+mrzcmbMcC8p8TceWebg/tvfhtEnnBAW39iYxg9JRHod4C1vZb+a9R17todMJxZ39332bvBlhVu5T5wYdti77+6+5ZbuBQXNe/0tt3T/7/92X7HC6+vdb7455JEFC9y9osL98svdhwwJ0x5+uPtvfuP+5z+7P/yw+8svu3/8sftTT7n37+++1VbuH3/sV18dJv/Vr0IOieeu/v3dzz3X/bPPPCS3W291Hz06vHnUUe733+8O3vSLX/ree7tvtllIkO7uf/97mGzevIx/bCLSgymxZDmx/Pa37qfwZ28qibpvvbX7tGnuxx/v/otfuN9wg/uzz7o3NHhTUyheJk70r4qXAQNCUnB397Iy94svbq5gkg077OBeWuqPP+5uFvJEYnXxr3+5H3dcyGl5ee4XXOBeX+8heV1yiXs0GpYzfLg/+1C5g/t11zXP/+WX4e3LL8/4xyYiPZgSS5YTyxtvhE/6rrtan+b5592nTg3TbbddKES++MJ90qSQAK69tsUMNTWhGe3tt91nzXL/299Cklq71hcsCFXJzju7b9yYfH1Ll7qfempY37Rp7itXxt5Ytsz9vPO86ZFHfeed3ceNc6+t3XTenXZy32efTn4YItInKLFkObE0NIRWrJNOCq+rqtzff9/9iSfc//hH9wMO8K+Ogdx6a6yCiKmocD/00PD+mWdu+l4yK1eGlrDRo0Peac/f/uYeibiPGeM+d27z+FhrmN9++9fnueAC96Ki1pOWiPR9SixZTizu7kce6V5SEnbgLVuvRoxwv/rqkHCSaWgIO3Nwnz7dvbw8+XQ1NeHwTSTi/uabqcf29tuhMikqcr/pppC8Jk503377sO6WZs0Kscyalfo6uqKhQcd0RHqathKL7iDZTc44A9avhy22gAkTwrD11uFxxIi2OwvOzw9nCE+cGJbzf/4PTJsWbs3dr1/z43PPhctV7r0XpiS9YWhykyfDvHlw7LFw6qlwyy3hjOWHHkp+d+E99ww95j/9NPzbv3X4o+iwK64I9xZ7/nn43vcyvz4R6Rrd8z7d97zPsOeeg3POCbdpr6yE6upN3/+v/4Jf/apzy25shIsvDvfxmjo1XDrTWsLbd98Qw4IFnVtXqtauDcl3wwbYe2+YMyez6xOR1Oie933IvvvCe+81v25sDBfqb9wY7hq8+eadX3Z+frjY/tBDYezYtquoAw6AmTNhxQoYNarz62zPFVeEBHr66XDjjfDCCyHBiEjPpSvve7n8fBgwIOzcu5JUEk2d2v6y9t8/PD77bMeX/8UXoZeZ9pSWwnXXwfHHh/uLjRoFl17a8fVJ7snxhpisU2KRTpk0CYYPD8dZOqKqCqZPh8MOC12VteXSS8MO4pJLoKQk3Gts9mx48cXORi254NFHYcstM99MK61TYpFOycsLzXLPPtuxX4cXXAAffhi6KDvlFPjXv5JP99FHcNtt8NOfwrhxYdxpp8Fmm6lqkdZVVcHZZ4dq94QToK4u2xHlJiUW6bQDDoAvv9z0mE9bHn44HCf5f/8vnOE1bBj84AfhAH1Lv/oVRKPwi180j4tXLc8/Dy+/nJ5tkL7lyitDx94XXgjvvBOOGUoWtHYecq4M3XUdS1/0xRfhepbf/779aZcudR86NPQGEL+S//XXw7Uz++236YWfb74ZlnvxxV9fzsaNoe+y/fZLyyb0SI2NoRu4U08N1yZJahYvDtdwHX10eH3CCe75+aHnC0k/dIGkEkumTJwYLtpsS0OD+/e+596vX+grM9Gtt4a/wp//vHnc/vuHHpRbuxA03rnmyy8nf/+TT9xnznS/++7mzjPbim32bPcLLww9IWTbu++GjqbjF89ecEG2I+o9jjoqXIT8xRfh9fr14YLk7bZzr67Oamh9khKLEkvGnHVW+Gdu65f1f/93+Ev7y1+Sv//Tn4b377039McJ4W4BramsDL0V7L//puMrKkJCKSpq3jGXlDR32BzvgqaxMSSls892HzWqeVpwP+SQWK/P3WzjxhB7QYH7sGGhK53TT/du7eHA3X3dOvef/MT92GPdf/lL91tuCd/JokXudXXdF0dHzZ4dPqtLL910fLyXiJ/9LCthtet3v3O/7DL3pqZsR9JxSixKLBnz2GPhr+j555O/P3duaI44+ujW/3lqa92/+93QsfJ227lvsUX7vzB/97uw3ldfDcu94w73zTcP4044IfST9sIL7mec4T5yZBjfr1/od23s2PA6EnH/wQ9CQlu/PiyzX78w/tJLu+9X7qxZ7uPHh5hOOsl99eowvqoqdFa92WYJnYRm0Jo17pMnh161x40L31ti0s3PD59fR7oL6g4NDaFj1C23TN4t0mmnhZ6+X3qp+2NryzXXNH+2P/1p77vHkRKLEkvGbNgQfmWfdZb755+HHfqqVaGH/1Wr3CdMCP/w69e3vZzly5tvCXPbbe2vN1617Lpr6B8NwvGbxHuVxTU0hMR32mmhg85DDgmJaMOGr0+7dGlIghBiz2Tz2Lp14RYGEJoUZ8/++jQLFrgXF7sfeGBmdzwrV4Z7vhUXN9+moa4uVG/PPx+aLC+4oPmWQAcd5P7aa5mLpyNuuCHEdN99yd/fsCEkyq23br9ptLs88EBIdocfHpqBwf3kk5P3zddTKbEosWTUXnt5q7eHyctr/VhIS2+/HZpf2uvBOe7KK/2rTjxvuSW9O95nn3Xfdlv/qnls0aL0Lds97Lw33zwk5V//uu2mxOuv93abB7ti+fJQKZaUuD/zTNvTlpeHe/EMGxZiOuCA1L/fTFi3LsSy995tNyfFm8rOPLO7ImvdK6+Eqni33UKF1dQU/gYg3KYp1b//bFNiUWLJqMWLQwXwl7+Es5muuy6cKXbFFe5PP5259dbWhvW2Vw11ZflXXhmax4qLw1lqrfVAnary8vDLNH5PtlR6bW5qcj/ssNBE1d6dqTuqtNT9m98M2zhnTurzVVSE21zH7zk3cWJogrz++nB37O46HnPOOeHHy/z57U977rn+1V24r7jC/cMPMx7e13z4YTg7cpttmps8437zmxDf0Uf37ONZcW0lFnVC2cs6oZTut2xZuPbm7rvDxZrXXBP6U2urL7VknnsOfvKTcPHez38eehQoLk5t3jVr4NvfDt33zJsXerPuqiVLYJ99QmeiTz0Fu+/e8WVs3Ah/+Qs88wy8/jqsXBnGRyLwne/A+PFhG4uKwhB/XlcX1rtmTRjizxsaQtc9LYfBg6G+Pgx1dWGoroY//CFcaHvDDe3HWlMTugZ68EF4++0wbuLE0AvEoYeGeEtKOv4ZpGrlSthtt9D33Wuvhd7NW/rd78LfxhFHwD33hM+qM9zD9zt3LixeDDvuCLvsEi4wTpe2OqFUYlFikRTNmQNnnRUuCJ0+HY4+OlzpHe8ENPGxujoM8eeVlTB/Pnzzm3D77bDrrh1f/+zZobeDI4+Egw7adPnV1WFnO2AADBrUPAweHC40XbMGli8PF7QuXx6GN94IO9unnw79w3WVe+gH7vXXm4cVK6C2tjkZxJ8XFITbRQwf3vw4fHjo+27lyjDfihUh3rKy5OsrKgqf5+zZYd6OiPdX9+ij4XttaAg/FMaPD71CbL89bLddWP6GDWEnvWRJmG/JknARZkEBDBkSPuMhQ5qfjxoVbo8xdmwYRo8On/O0aeFvZ86ctj/vP/4RzjsvxDJ8OBQWhm0tLAxDNAojR4ZhxIjm501N4TOfOzcMK1Z8fdlbbBESzC67hFtr7LZb53+kKLG0QYlFOqK+Hq6/PtxeYMOGTd+LRMI/fXwoKWl+LCkJ97256KIwrrN+/Wv4zW++Pj4/P+x0amraX8aIEaGT0S23DN3jTJ7c+Xi6Q00NVFQ072CLisL2drRibE1ZWagmFy6E99+HDz4IXQq17A6moCAkiq22CjvopqZwj6WWQ339pvPl54d7JlVUwCOPwCGHtB/TnXfCffc1V2iJj5WVocJbty75vNtsE3647LprSBzjx8O778KbbzYPixaFaV9/vfM/KpRY2qDEIp2xYUOoAvr1a04kyW6KlgmffRZ2qomJq7AwvNfQEGIrK4Py8jBs3Bh++Y4eHX5Nd7Z5JZc0NITP+ZNPQuW31VYhGbf3HbuH5FJaGqqa0tIwLFsG3/9+6MIoXerrQ3dIq1aFRNPQADvvnFr1tm5daFLda6/Um2NbUmJpgxKLiEjHtZVY+lQnlGY2wcxuNbMHsh2LiEiu6vGJxcxuM7NVZrawxfjpZvaRmX1qZjMB3P0zdz85O5GKiAj0gsQC/BWYnjjCzPKB64EDge2BY8xs++4PTUREWurxicXdXwRanv8wFfg0VqHUAfcAh6W6TDM71czeMrO3Vq9encZoRUSkxyeWVowBlia8LgXGmNkwM7sRmGxmF7U2s7vf5O5T3H3KiBEjMh2riEhOKch2AJ2U7Ax2d/e1wOndHYyIiDTrrRVLKbBFwuuxwPIsxSIiIgl6a2J5E9jGzMabWREwA3gsyzGJiAi94AJJM7sbmAYMB1YCF7v7rWZ2EHANkA/c5u6/7eTyVwNLOhnecGBNJ+ftrbTNuUHb3Pd1dXu3cvekB6l7fGLpyczsrdauPO2rtM25Qdvc92Vye3trU5iIiPRQSiwiIpJWSixdc1O2A8gCbXNu0Db3fRnbXh1jERGRtFLFIiIiaaXEIiIiaaXE0knJuu3vrcxssZm9a2bzzeyt2LihZvaMmX0SexySMP1Fse3+yMz+LWH8zrHlfGpm15ql6+axXZfs9gvp3EYzKzaze2PjXzezcd26gUm0ss2XmNmy2Hc9P3Y9WPy9Xr3NZraFmc02sw/M7D0zOzc2vs9+z21sc3a/Z3fX0MGBcFHmImACUAS8A2yf7bi6sD2LgeEtxl0FzIw9nwlcGXu+fWx7i4Hxsc8hP/beG8BuhL7cngIOzPa2JWzPXsB3gIWZ2EbgDODG2PMZwL09dJsvAX6WZNpev83AaOA7secDgI9j29Vnv+c2tjmr37Mqls7pUrf9vcRhwO2x57cDhyeMv8fda939c+BTYKqZjQYGuvtrHv4C/5YwT9Z58tsvpHMbE5f1ALBvtiu2Vra5Nb1+m939S3d/O/a8AviA0BN6n/2e29jm1nTLNiuxdE7SbvuzFEs6OPC0mc0zs1Nj4zZz9y8h/PECI2PjW9v2MbHnLcf3ZOncxq/mcfcGoBwYlrHIu+YsM1sQayqLNwv1qW2ONddMBl4nR77nFtsMWfyelVg6J2m3/d0eRfrs7u7fIdyR80wz26uNaVvb9r70mXRmG3vL9t8AbA1MAr4E/ic2vs9ss5n1Bx4EznP3DW1NmmRcX9nmrH7PSiyd06e67Xf35bHHVcDDhKa+lbHymNjjqtjkrW17aex5y/E9WTq38at5zKwAGETqzVDdxt1XunujuzcBNxO+a+gj22xmhYQd7J3u/lBsdJ/+npNtc7a/ZyWWzukz3fabWT8zGxB/DhwALCRsz4mxyU4EHo09fwyYETtTZDywDfBGrImhwsx2jbW/npAwT0+Vzm1MXNaRwPOxtuoeJb6DjTmC8F1DH9jmWHy3Ah+4++8T3uqz33Nr25z17zmbZzT05gE4iHAGxiLgl9mOpwvbMYFwlsg7wHvxbSG0oT4HfBJ7HJowzy9j2/0RCWd+AVNif8CLgP8l1rNDTxiAuwlNAvWEX2Anp3MbgQhwP+Fg6BvAhB66zX8H3gUWxHYYo/vKNgN7EJpoFgDzY8NBffl7bmObs/o9q0sXERFJKzWFiYhIWimxiIhIWimxiIhIWimxiIhIWimxiIhIWimxiPQyZjbNzJ7IdhwirVFiERGRtFJiEckQMzvOzN6I3Q/jz2aWb2aVZvY/Zva2mT1nZiNi004ys7mxTgMfjncaaGbfMLNnzeyd2Dxbxxbf38weMLMPzezOhHtnXGFm78eWc3WWNl1ynBKLSAaY2XbA0YQOPicBjcCPgX7A2x46/XwBuDg2y9+AC919J8IV0/HxdwLXu/u3ge8SrqSH0IvteYT7a0wAdjezoYTuO3aILeeyTG6jSGuUWEQyY19gZ+BNM5sfez0BaALujU1zB7CHmQ0CBrv7C7HxtwN7xfpwG+PuDwO4e427V8WmecPdSz10MjgfGAdsAGqAW8zsB0B8WpFupcQikhkG3O7uk2LDRHe/JMl0bfWp1NbNlGoTnjcCBR7ulTGV0NPt4cCsjoUskh5KLCKZ8RxwpJmNhK/uu74V4X/uyNg0xwIvu3s5sN7M9oyNPx54wcN9NUrN7PDYMorNLNraCmP35Bjk7k8SmskmpX2rRFJQkO0ARPoid3/fzP6TcGfOPEIPw2cCG4EdzGwe4U58R8dmORG4MZY4PgNOio0/Hvizmf1XbBlHtbHaAcCjZhYhVDvnp3mzRFKi3o1FupGZVbp7/2zHIZJJagoTEZG0UsUiIiJppYpFRETSSolFRETSSolFRETSSolFRETSSolFRETS6v8D9g32h/MUtYIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Calulate relative error based on network MSE Loss. '''\n",
    "train_rel_error = np.sqrt(train_loss_sto/denom)\n",
    "val_rel_error = np.sqrt(val_loss_sto/denom)\n",
    "\n",
    "''' Plot results. '''\n",
    "input_label = 'epochs'\n",
    "output_label = 'relative error'\n",
    "title = 'NACA drag'\n",
    "\n",
    "plt.close()\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(epoch_sto,train_rel_error,'b',label='train')\n",
    "plt.plot(epoch_sto,val_rel_error,'r',label='val')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(input_label)\n",
    "plt.ylabel(output_label)\n",
    "plt.title(title)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--bibtex\n",
    "\n",
    "@misc{bollinger2020reduced,\n",
    "      title={Reduced Order Modeling using Shallow ReLU Networks with Grassmann Layers}, \n",
    "      author={Kayla Bollinger and Hayden Schaeffer},\n",
    "      year={2020},\n",
    "      eprint={2012.09940},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG}\n",
    "}\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "<a name=\"cite-bollinger2020reduced\"/><sup>[^](#ref-1) </sup>Kayla Bollinger and Hayden Schaeffer. 2020. _Reduced Order Modeling using Shallow ReLU Networks with Grassmann Layers_.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
